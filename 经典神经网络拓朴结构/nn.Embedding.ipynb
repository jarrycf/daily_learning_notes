{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn \n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "1 embedding 的计算过程\n",
    "\n",
    "nn.Embedding 实现了什么?\n",
    "在起前向过程中实现了一个查表\n",
    "\n",
    "表的形式是怎么样的? \n",
    "matrix (embedding.weight, learnable parameters)\n",
    "martix.shape (v, h)\n",
    "    - v: vocabulary size\n",
    "    - h: hidden dimension\n",
    "\n",
    "查表的过程是如何实现的? \n",
    "input: (b, s)\n",
    "    - b: batch size\n",
    "    - s: seq len\n",
    "(b,s) 和 (v,h) =>? (b,s,h)\n",
    "one-hot + 矩阵乘法\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.4147,  0.9515, -0.0327],\n",
      "        [-0.0169,  0.1072,  1.0722],\n",
      "        [ 1.2686,  0.7146,  1.2683],\n",
      "        [ 0.1776,  0.7265, -0.8443],\n",
      "        [ 0.6266,  0.5764,  0.8150],\n",
      "        [ 0.1367,  2.0113,  0.3402],\n",
      "        [ 0.3035,  0.1572, -0.3762],\n",
      "        [ 0.9478,  2.1515,  0.9349],\n",
      "        [ 0.5556,  0.2157,  0.6702],\n",
      "        [ 1.4950,  1.4584, -0.1224]], requires_grad=True)\n",
      "torch.Size([10, 3])\n",
      "torch.float32\n",
      "torch.int64\n",
      "tensor([[[-0.0169,  0.1072,  1.0722],\n",
      "         [ 1.2686,  0.7146,  1.2683],\n",
      "         [ 0.6266,  0.5764,  0.8150],\n",
      "         [ 0.1367,  2.0113,  0.3402]],\n",
      "\n",
      "        [[ 0.6266,  0.5764,  0.8150],\n",
      "         [ 0.1776,  0.7265, -0.8443],\n",
      "         [ 1.2686,  0.7146,  1.2683],\n",
      "         [ 1.4950,  1.4584, -0.1224]]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "embedding = nn.Embedding(10, 3)\n",
    "print(embedding.weight)\n",
    "print(embedding.weight.shape)\n",
    "print(embedding.weight.dtype)\n",
    "\n",
    "input = torch.LongTensor([[1, 2, 4, 5], [4, 3, 2, 9]])\n",
    "print(input.dtype)\n",
    "\n",
    "print(embedding(input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "         [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "         [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
      "         [0, 0, 0, 0, 0, 1, 0, 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
      "         [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
      "         [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]]])\n",
      "torch.Size([2, 4, 10])\n",
      "torch.int64\n"
     ]
    }
   ],
   "source": [
    "input_onehot = F.one_hot(input, num_classes=10)\n",
    "print(input_onehot)\n",
    "print(input_onehot.shape)\n",
    "print(input_onehot.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.5273,  0.8237,  0.8261],\n",
       "         [-1.0407, -0.3688,  1.2705],\n",
       "         [-0.2796,  0.6942, -0.6633],\n",
       "         [ 0.3586,  1.0373, -2.2510]],\n",
       "\n",
       "        [[-0.2796,  0.6942, -0.6633],\n",
       "         [-0.5626, -0.2823,  0.1150],\n",
       "         [-1.0407, -0.3688,  1.2705],\n",
       "         [ 1.8176,  0.7679, -1.3234]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# input_one.shape: (b, s, v)\n",
    "# embedding.weight.shape: (v, h)\n",
    "torch.matmul(input_onehot.type(torch.float32), embedding.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n(b,s) 和 (v,h) ->? (b,s,h)\\n(b,s) 经过 one-hot => (b, s, v)\\n(b, s, v) @ (v, h) => (b, s, h)\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "(b,s) 和 (v,h) ->? (b,s,h)\n",
    "(b,s) 经过 one-hot => (b, s, v)\n",
    "(b, s, v) @ (v, h) => (b, s, h)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "2 max_norm\n",
    "max_norm的作用是什么?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0982, grad_fn=<MeanBackward0>)\n",
      "tensor(1.0048, grad_fn=<StdBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[ 0.8402, -0.2582,  0.5306,  1.2442,  0.9572],\n",
      "        [ 2.4128, -0.6782, -0.1286,  0.1715,  0.1777],\n",
      "        [-0.5644, -0.9521, -1.0977,  0.2060, -1.3885]], requires_grad=True)\n",
      "tensor([1.8758, 2.5217, 2.0977], grad_fn=<NormBackward1>)\n",
      "torch.Size([3])\n",
      "tensor([[ 0.8402, -0.2582,  0.5306,  1.2442,  0.9572],\n",
      "        [ 2.4128, -0.6782, -0.1286,  0.1715,  0.1777],\n",
      "        [-0.5644, -0.9521, -1.0977,  0.2060, -1.3885]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n",
      "tensor([1.8758, 2.5217, 2.0977], grad_fn=<NormBackward1>)\n"
     ]
    }
   ],
   "source": [
    "# 不设置 max_norm, max_norm == False 时, 特征粒度(所有元素粒度)的范数\n",
    "# 范数: 向量的长度. 将一个向量中的每个元素取绝对值后，再将这些绝对值求和并开方\n",
    "embedding = nn.Embedding(3, 5)\n",
    "print(embedding.weight.mean())\n",
    "print(embedding.weight.std())\n",
    "# 01高斯分布采样得到\n",
    "\n",
    "print(embedding.weight)\n",
    "print(torch.norm(embedding.weight, dim=1))\n",
    "# 行的粒度上的norm是随机的\n",
    "\n",
    "input = torch.tensor([0, 1, 2])\n",
    "print(input.shape)\n",
    "output = embedding(input)\n",
    "print(output)\n",
    "print(torch.norm(embedding.weight, dim=1)) # 计算L2范数\n",
    "'''\n",
    "Frobenius范数: 将矩阵中所有元素平方后相加再开方\n",
    "L1范数: 将向量中所有元素的平方相加再开方\n",
    "L2范数: 正则化\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1618, grad_fn=<MeanBackward0>)\n",
      "tensor(1.2941, grad_fn=<StdBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[-0.7501, -1.5564, -0.1468,  0.4527,  0.9336],\n",
      "        [ 0.8889, -1.0233,  0.4655, -0.2099, -2.0676],\n",
      "        [ 1.0238,  1.6944, -0.8625,  2.8629,  0.7222]], requires_grad=True)\n",
      "tensor([2.0207, 2.5245, 3.6580], grad_fn=<NormBackward1>)\n",
      "torch.Size([3])\n",
      "tensor([[-0.3712, -0.7702, -0.0727,  0.2241,  0.4620],\n",
      "        [ 0.3521, -0.4054,  0.1844, -0.0832, -0.8190],\n",
      "        [ 0.2799,  0.4632, -0.2358,  0.7826,  0.1974]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], grad_fn=<NormBackward1>)\n"
     ]
    }
   ],
   "source": [
    "# max_norm == True ==> max_norm == 1\n",
    "embedding = nn.Embedding(3, 5, max_norm=True)\n",
    "print(embedding.weight.mean())\n",
    "print(embedding.weight.std())\n",
    "\n",
    "print(embedding.weight)\n",
    "print(torch.norm(embedding.weight, dim=1))\n",
    "\n",
    "input = torch.tensor([0, 1, 2])\n",
    "print(input.shape)\n",
    "output = embedding(input)\n",
    "print(output)\n",
    "print(torch.norm(embedding.weight, dim=1))\n",
    "# max_norm == True 会对输出的结果进行一次norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6 (default, Aug  5 2022, 15:21:02) \n[Clang 14.0.0 (clang-1400.0.29.102)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "16fd2a30cafabffcf987353b6f32a69c149f14990ff457e624dc4546598fcb44"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
